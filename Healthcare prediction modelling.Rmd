---
title: "Theming with bslib and thematic"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_float: 
      collapsed: false
      smooth_scroll: true
    theme:
      bg: "#F9F7F7"
      fg: "#112D4E"
      primary: "#DBE2EF"
      secondary: "#3F72AF"
      base_font:
        google: Prompt
      heading_font:
        google: Proza Libre
---

```{r setup, include=FALSE}
if (!require(pacman)) {install.packages("pacman")}

pacman::p_load(plyr, broom, kableExtra, rstatix, rstatix, MASS, ggmosaic, DT, rpart, rpart.plot, glmnet, randomForest, ranger, gbm, xgboost, tidyverse)


# Import data
healthcare <- read_csv("Hospitals_Train.csv")


```



# Introduction 

This is my attempt at selecting the best classification model out of logistic regression and decision tree options for the healthcare prediction data that was analyzed by [`ALIA107`](https://www.kaggle.com/code/bvc5283/healthcare-prediction/notebook). 
The data was downloaded from here: https://github.com/HannahHan3/758T_PredictiveModeling

I will by guided by the following aims:

-   to refine the code
-   to employ a more detailed EDA
-   to look for possible interaction effects

The goal is to predict whether a discharged patient returns to the hospital within 30 days.




# Infrastructure

First, I create a set of vectors and functions that will help in automating the code in the context of producing tables and plots.

## Plots


### Color Palette

```{r}
# https://colorhunt.co/palette/f9f7f7dbe2ef3f72af112d4e
#F9F7F7
#112D4E
#DBE2EF
#3F72AF

colors <- c(
  "#3468C0",
  "#FF9843",
  "#FFDD95",
  "#86A7FC"
)


thematic::thematic_rmd(font = "auto", qualitative = colors)




```



## Tables


```{r}

own_table <- function(x) {
  x %>% 
    kable() %>% 
    kable_minimal(lightable_options = "hover")
}

```


## Functions


```{r}

class_error <- function(x, y) {
  
  mean(if_else(x >= 0.5, "Yes", "No") != y)
  
}



```



# Data cleaning

Let's take a look at the basic structure of the dataset.

```{r}

glimpse(healthcare)


```

The `read_csv` function, as opposed to `read.csv`, automatically ascribes `NA` to empty values.


```{r}

dimen <- dim(healthcare)
dimen

```
The raw dataset contains `r dimen[1]` observations with `r dimen[2]` variables.

In order to ease coding, the names of all variables are converted to lowercase.

```{r}

healthcare <- healthcare %>% 
  rename_with(~str_to_lower(.), everything())

```



## Missing data


```{r}
healthcare %>% 
  summarise(
    across(everything(), list(
      number = ~sum(is.na(.)), 
      percent = ~mean(is.na(.)) 
      )
    )
  ) %>% 
  pivot_longer(
    everything(), 
    names_pattern = "(.*)_(.*)",
    names_to = c("Variable", "Statistic"),
    values_to = "value"
    ) %>% 
  pivot_wider(names_from = Statistic, values_from = value) %>% 
  mutate(percent = round(percent * 100, 1)) %>% 
  arrange(desc(percent)) %>% 
  filter(number != 0) -> missing

missing2 <- missing %>% 
  filter(percent > 10)

missing %>% 
  own_table
```


There are `r nrow(missing)` variables with missing data. However, only `r nrow(missing2)` have a noticeably high number of missing values, while the remaining stay below a level of 10%.

We can check, if there is any pattern in the missing values in relation to the `return` (DV) variable.


```{r}

missing_filt <- missing %>% 
  pull(Variable)


healthcare %>% 
  select(return, all_of(missing_filt)) %>% 
  mutate(across(-return, ~if_else(is.na(.), "Missing", "Non-missing"))) %>% 
  pivot_longer(-return) %>% 
  nest(.by = name) %>% 
  mutate(
    freq_table = map(data, freq_table, value, return)
  ) %>% 
  select(-data) %>% 
  unnest() %>% 
  ggplot(aes(value, prop, fill = return)) +
  geom_bar(stat = "identity") +
  facet_wrap(~name)


```


Differences in the proportions of those who returned were observed across variables with missing values. Therefore, incorporating information about these missing values could enhance the model's performance. Consequently, for character variables, `NA` will be coded as *'Missing'*.

As `consult_in_ed` is a numerical variable, it is methodologically incorrect to include a character value such as *'Missing'*. However, let's inspect its unique values.

```{r}

unique(healthcare$consult_in_ed)
  

```
This likely indicates that a `1` represents the occurrence of consultation, while `NA` represents the absence of such an occurrence.
However, having no definite knowledge, it is prudent to label `NA` as *'Missing'*. On a mathematical level, the choice of labeling won't affect the model.





```{r}
# Adding missing as a string value
healthcare <- healthcare %>% 
  mutate(
    across(where(is.character), ~replace_na(., "Missing")),
    consult_in_ed = mapvalues(consult_in_ed, c(1, NA), c("Consulted", "Missing")),
    consult_in_ed = factor(consult_in_ed)
  ) 
  
```




## Categorical variables


### Large qualitative variables


```{r}

qualitatives <- healthcare %>% 
  select(where(is.character)) %>% 
  map(unique) %>% 
  map(length) %>%  
  enframe() %>% 
  unnest() %>% 
  arrange(desc(value))
  
qualitatives %>% 
  own_table()
  

```

One character variable, `r qualitatives$name[1]`, has `r qualitatives$value[1]` unique values.
However, given its name, it seems to be a continuous variable.


```{r}
set.seed(1)
healthcare %>% 
  slice_sample(n = 20) %>% 
  pull(charges)


```
Its values, together with its name, almost certainly indicate that it is a continuous variable, representing how much money a given patient was charged for being admitted to the hospital.


```{r}
healthcare %>% 
  mutate(
    charges2 = as.double(charges)
  ) %>% 
  filter(is.na(charges2)) %>% 
  select(charges, charges2) %>% 
  datatable()
```

A small portion of the observations for that variable has uninterpretable values. Nothing to worry about.


```{r}
# Apply modification of the charge variable to the dataset
healthcare <- healthcare %>% 
  mutate(
    charges = as.double(charges)
  ) %>% 
  drop_na(charges)

```



### Unique values

Checking what character vectors represent by inspecting unique values.

```{r}

healthcare %>% 
  select(where(is.character)) %>% 
  map(unique) %>% 
  keep(~length(.) < 50) %>% # Exclude potential qualitative variables 
  {
    by_length <- lengths(.) %>% 
      order()
    .[by_length]
    }


```
Variables like Race or Ethnicity have two values that essentially mean the same thing - *"Declined to answer"* and *Unknown*". Both will be transformed to missing values.

The abbreviations "**DC**" and "**ED**" stand for "discharge" and "emergency department", respectively.
Both consist of a substantial number of unique values. 
Incorporating them as is into some models, such as regressions, would lead to significant penalization due to the larger number of predictors.
Moreover, aiming for higher interpretability favours a simpler model.
Therefore, we will simplify those variables into a fewer number of levels. This will extend to `race` and `financial_class` as well, as they also feature more than 10 levels.

```{r}
healthcare %>% 
  select(financial_class, ed_result, dc_result, race) %>% 
  pivot_longer(everything()) %>% 
  nest(.by = name) %>% 
  mutate(freq = map(data, freq_table, value)) %>% 
  select(-data) %>% 
  unnest() %>% 
  arrange(name, desc(prop)) %>% 
  filter(prop > 1) %>% 
  own_table()

```


Based on the observed proportions, `dc_result` will be recoded into a variable with only two values, reflecting that the majority of the patients fell into one category. 
The `ed_result` and `financial_class` variables, which display a more varied distribution, will be consolidated such that categories representing more than 5% of observations will remain unchanged, while the rest will be grouped into an 'other' category.
While a more theoretically-oriented approach could be applied to these variables, it would necessitate greater domain-specific knowledge. 
 `race` will be divided into three categories:
*"Black or African American"*, 
*"White"* and 
*"Other"*.


```{r}
# Check if there are other, similarly formulated values as "home or self care"
healthcare$dc_result %>% 
  unique() %>% 
  str_subset("[hH]ome|[sS]elf")
  

```
"Home Health Care Svc" appears similar to "Home or Self Care". However, due to the distinction between them not being clear and my lack of sufficient theoretical understanding, they will be maintained as separate categories.




```{r}
healthcare <- healthcare %>% 
  mutate(
    across(c(ed_result, financial_class), ~fct_lump_prop(., 0.05)),
    race = fct_lump(race, 2),
    dc_result = fct_lump(dc_result, 1)
  ) 
  


```




Variable `acuity_arr` has a suspected value *"5 Purple"*, as it does not make sense in the context of the remaining values.

```{r}
healthcare %>% 
  count(acuity_arr) %>% 
  own_table()


```

As there is only one observation with such value, it can be safely ignored as `NA`.

```{r}


# Transforming remaining character vectors to factors
healthcare2 <- healthcare %>% 
  mutate(
    acuity_arr = mapvalues(acuity_arr, "5 Purple", "Missing" ),
    acuity_arr = factor(acuity_arr) %>% fct_relabel(~str_replace(., "[0-9]-", "")),
    across(c(race, ethnicity), ~plyr::mapvalues(
      .,  
      from = c( "Declined to Answer","Unknown"),
      to = c("Missing", "Missing")
      )
    ),
    across(
      c(hospital, gender, race, ethnicity, diagnosis, return, financial_class, admit_result),
      ~as.factor(.)
      ),
    across(c(risk, severity), ~factor(., levels = c("Minor","Moderate", "Major", "Extreme", "Missing"))),
    across(
      c(race, financial_class, admit_result, ethnicity), 
      ~fct_relevel(., "Missing", "Other", after = Inf)) # Move "other" to the end
  ) 





```



## Numerical variables



```{r}
# Inspect the 10 first unique elements 
healthcare2 %>% 
  select(where(is.numeric)) %>% 
  map(unique) %>% 
  map(function(x) {x[1:10] %>% keep(!is.na(.))}) 

```

Three variables are binary which suggests that they are of *occurred* vs *not occurred* nature and as such are categorical. They will be transformed to factor variables.

Six of the numerical variables are date / time objects.
They could be treated as either categorical or continuous variables, depending on the theoretical and methodological backgrounds. In this case, as our outcome variable is binary and there is no apparent and theoretically grounder reason to expect a linear aspect to it in terms of passing time, it is more methodologically sound to treat the date / time variables as factors, whereby the model would assess the effect of each time-point separately. 
This decision will be further explored as part of EDA.


```{r}
# Transforming selected numerical columns to factors
healthcare2 <- healthcare2 %>% 
  mutate(
    across(c(same_day, consult_order, consult_charge), ~as.factor(.)),
    across(matches("weekday|month|hour"), ~as.factor(.))
    )



```




# EDA

## Return - the outcome variable


```{r}
healthcare2 %>% 
  freq_table(return) %>% 
  own_table()

```

For every three patients who did not return, there was one who did.


## Exploration of the categorical variables

```{r fig.asp=1.5}

# Pivot data and create a faceted bar plot
healthcare2 %>% 
  select(where(is.factor), -c(matches("hour|month|weekday"), return) ) %>% 
  pivot_longer(everything()) %>% 
  nest(.by = name) %>% 
  mutate(freq = map(data, freq_table, value)) %>% 
  unnest(.by = freq) %>% 
  mutate(name = factor(name) %>% fct_relevel("return", after = Inf)) %>% # Show outcome variable as last
  ggplot(aes(value, prop)) +
  geom_bar(stat = "identity") +
  facet_wrap(~name, scales = "free", ncol = 3) +
  coord_flip()


```

Besides what was already learned while inspecting variables with missing values, we can see equal distributions of categories in gender and the hospital where the patient was admitted. 
Interestingly, the frequency of those who were diagnosed is similar to the frequency of those who did not return. 
Theoretically, it would make sense for the diagnosis to be a strong predictor of return, as knowing the nature of the disease would allow for proper and quicker treatment, leading to resolution. 



```{r fig.asp=1.5}

# Pivot data and create a faceted bar plot of all string variables against return
healthcare2 %>% 
  select(return, where(is.factor), -matches("hour|month|weekday")) %>% 
  pivot_longer(-return) %>% 
  nest(.by = name) %>% 
  mutate(
    freq = map(data, freq_table, value, return)
    ) %>% 
  unnest(.by = freq) %>% 
  ggplot(aes(value, prop, fill = return)) +
  geom_bar(stat = "identity") +
  facet_wrap(~name, scales = "free", ncol = 2) +
  coord_flip()


```

Diagnosis, or its lack to be more specific, indeed seems to be a predictor of return. However, the relationship is not as strong as anticipated. 

Regardless of the hospital, the proportion of those who returned within 30 days to the hospital was almost the same. 

Interestingly, in almost all variables that had `NA`, missing value was associated with a higher chance of return.



## Exploration of continuous variables

### Descriptive statistics and distribution of age, charges, and diagnosis details


```{r}

descriptives <- healthcare2 %>% 
  get_summary_stats(type = "common") 

# Extract stats for an automated describing 
descriptives2 <- descriptives %>% 
  nest(.by = variable) %>% 
  mutate(data = map(data, unlist)) %>% 
  pull(data) %>% 
  set_names(descriptives$variable)

descriptives %>% 
  own_table()

```


People were between `r descriptives2$age[2]` and `r descriptives2$age[3]` years old with a mean of `r descriptives2$age[6]`.

Charges encompassed from `r descriptives2$charges[2]` to `r descriptives2$charges[3]`. As the mean (*M* = `r descriptives2$charges[6]`) was substantially bigger compared to the median `r descriptives2$charges[4]` this suggests a large number of extreme values and that the distribution was rightly skewed.




```{r}

healthcare2 %>% 
  select(where(is.numeric), -index) %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(value)) +
  geom_histogram() +
  facet_wrap(~name, scales = "free") 
  


```

Most of the patients were either in their early adulthood or middle-age.

Charges displays a pareto distribution, which makes sense, as most cases should be mild to moderate without the need for life-saving, expensive treatment, like invasive surgeries. 


```{r}

# Add breaks and labels for the intervales of the charges' variable
breaks <-  c(seq(0, 10000, 1000), sd(healthcare2$charges), max(healthcare2$charges))
for_breaks <- format(breaks, scientific = F)

healthcare2 <- healthcare2 %>% 
  mutate(
    charges_int = cut(
      charges, 
      breaks = breaks,
      labels = str_c(for_breaks[-length(for_breaks)], for_breaks[-1], sep = " -")
    )
  ) 

healthcare2 %>% 
  freq_table(charges_int) %>% 
  mutate(cum_prop = cumsum(prop)) %>% 
  own_table()


```


Majority of the patients (75%) were charged between 0 and 4000 thousand dollars. 

Still, a noticeable number of patients were charged above 10k, with 5% even reaching more than 28k dollars.




```{r}

healthcare2 %>% 
  ggplot(aes(age, color = return)) +
  geom_density() 
  

```

Being between 50 and 60 years old was associated with a higher chance of return.
This was somewhat reversed for people above 65 years old and, to a lesser degree, between 25 and 35.




```{r}
healthcare2 %>% 
  freq_table(charges_int, return) %>% 
  ggplot(aes(charges_int, prop, fill = return)) +
  geom_bar(stat = "identity") +
  coord_flip()


```

Generally speaking, the smaller the charge, the higher the chance of return. 
However, the rates of return were relatively similar for patients who were charged between 1k and 28k dollars. The biggest differences were observed for those who were charged either very little (up to 1k) or a lot (above 28k).


## Exploration of the date / time variables

```{r}

healthcare2 %>% 
  select(matches("hour|month|weekday")) %>% 
  pivot_longer(everything()) %>% 
  count(name, value) %>% 
  mutate(value = factor(value, levels = 0:23)) %>% # Set 0 as the starting point (hour variable)
  ggplot(aes(value, n)) +
  geom_bar(stat = "identity") +
  facet_wrap(~name, scales = "free", ncol = 2) 
  

```

The arrivals and departures were most frequent during late afternoon and late evening hours and least frequent during morning. 
The months with somewhat higher departures and arrivals were January (possibly associated with the New Year's Eve) and March to July. There is no data for October.

Interestingly, the corresponding distributions of arrivals and departures in pairs of variables look almost identical.

```{r}
# Identify different values between hour variables
setdiff(healthcare2$hour_arr, healthcare2$hour_dep)


```
Indeed, they are identical. This could mean that the time between their arrival and, following discharge, departure was always shorter than one hour. However, a more plausible explanation is that there is a methodological error in data preparation, as one would expect at least few instances of patients who arrived at the end of a given hour and left at the onset of the next.



```{r}

date_unique <- healthcare2 %>% 
  select(matches("month|weekday|hour")) %>% 
  pivot_longer(
    everything(), 
    names_pattern = "(.+)_(.+)", 
    names_to = c("date", "type"), 
    values_to = "value"
    ) %>% 
  group_by(type) %>% 
  mutate(id = row_number()) %>% 
  pivot_wider(names_from = "type", values_from = "value") %>% 
  mutate(
    duplicated = arr == dep
  ) %>% 
  filter(!duplicated) %>% 
  count(date) %>% 
  set_names("Data", "Frequency")

healthcare2 %>% 
  count(same_day) %>% 
  filter(same_day == 0) %>% 
  set_names("Data", "Frequency") %>% 
  add_row(date_unique) %>% 
  mutate(Data = c("Different date discharge", "Difference in month departure and arrival")) %>% 
  own_table()


```

Interestingly, in the case of `r date_unique$Frequency[1]` observations, months for arrival and departure  were different. However, this is a very small number and it's also inconsistent with the number from the`same_day` variable. 

In order to avoid multicollinearity and to establish consistency, departure date variables will be removed.


```{r}
healthcare3 <- healthcare2 %>% 
  select(-ends_with("dep")) %>% 
  rename_with(~str_replace(., "_arr", ""), matches("month|hour|weekday"))

```


Let's examine the distribution of returns based on date-time points.

```{r}



healthcare3 %>% 
  select(weekday, hour, month, return) %>% 
  pivot_longer(-return) %>% 
  nest(.by = name) %>% 
  mutate(freq = map(data, freq_table, value, return)) %>% 
  unnest(.by = freq) %>% 
  mutate(value = factor(value, levels = 0:24)) %>%  # Set 0 as the starting point for the hour variable
  filter(return == "Yes")  %>% 
  ggplot(aes(value, prop, group = 1)) +
  geom_bar(stat = "identity") +
  geom_line(color = colors[2], linewidth = 0.8) +
  facet_wrap(~name, scales = "free", ncol = 2) +
  ggtitle("Rates of patient returns based on month, weekday and hour of the day")
    


```

The highest numbers of returns were observed from night to morning, peaking in the early-morning hours (between 5 and 7 a.m.). The relationship is, however, non-linear, and as the daytime has a cyclic nature, its starting point is chosen arbitrarily. 
This, along with previous considerations, calls for operationalizing the hour of the day as a categorical variable.

The rates of returns were more or less the same throughout the year with the exception of September, which was associated with noticeably lower numbers of returns. During the week, Monday was associated with somewhat higher rate of returns compared to the rest of the week. 

This speaks for reducing the number of levels such to group neighbouring levels with similar rates of returns. This will reduce the number of predictors in the final model.


```{r}

healthcare3 <- healthcare3 %>% 
  mutate(
    month = factor(month, labels = month.abb[-10]),
    weekday = factor(weekday, labels = c(
      "Mon",
      "Tue",
      "Wed",
      "Thu",
      "Fri",
      "Sat",
      "Sun"
    )),
    hour_cat = cut(as.double(hour) - 1, breaks = c(
      -Inf, 2, 4, 7, 9, 12, 13, 23
    )),
    weekday_cat = cut(as.double(weekday), breaks = c(
      -Inf, 1, 2, 3, 7
    )),
    month_cat = cut(as.double(month), breaks = c(
      -Inf, 6, 7, 8, 9, 12
    ))
  )




```






## Interaction effects

Before testing specific models, we can inspect the effects of the 2-way interactions  predictors on return.


```{r}
# Create list with all combinations of categorical variables
combinations <- healthcare3 %>% 
  select(where(is.factor), -return) %>% 
  map(unique) %>% 
  keep(~length(.) <= 5) %>%  # We consider only variables with few levels for the sake of interpretability
  names() %>% 
  combn(2, simplify = F) %>% 
  map(function(x) {c(x, "return")})


# Iterate log-linear models using glm on each combination of 2 categorical variables with outcome
interactions <- combinations %>% 
  map(function(x) {count(healthcare3, pick(x))}) %>% 
  map(glm, formula = n ~ .^3, family = poisson) %>% 
  map(anova, test = "Chisq") %>% 
  map(
    function(x) {
      as.data.frame(x) %>% 
        set_names(c("df", "deviance", "resid_df", "resid.dev", "p")) %>% 
        mutate(
        variable = rownames(.),
        dev_perc = deviance / max(resid.dev) * 100
        ) %>% 
        filter(str_detect(variable, ".*:.*:")) %>% 
        select(variable, deviance, dev_perc, p) %>% 
        as_tibble()
  
    }
  ) %>% 
  reduce(add_row)


# Select the strongest interactions
interactions2 <- interactions %>% 
  filter(p < 0.001) %>% 
  arrange(desc(dev_perc)) %>% 
  mutate(across(where(is.numeric), ~round(., 3)))

datatable(interactions2)
```

Assuming *p* = 0.001 as the cut-off point, there were `r nrow(interactions2)` significant interaction effects. Statistical significance, of course, on its own, should be treated with caution, especially when dealing with large samples. In order to inspect deeper the effects of the interactions, we could obtain Cramer's V and, for 2x2 relationships, odds ratios, separately on the levels of the third variable. 




```{r}
# Create odds ratio function
odds_ratio <- function(x) {
  # Turn a 2-column df to table
  tab <- table(x)
  
  # Obtain odds ratio only for 2x2 tables
  if (all(dim(tab) == 2)) {
  odds <- (tab[2, 2] / tab[2, 1]) / (tab[1, 2] / tab[1, 1]) 
  odds <- round(odds, 2)
  } else {odds <- NA} 
  return(odds)
}

# Create cramer's v function for iterated analysis
cramer_map <- function(x) {
  round(cramer_v(x[[1]], x[[2]]), 3)
}



# Extract odds ratio for all interaction effects
effects <- interactions2 %>% 
  pull(variable) %>% 
  map(str_split_1, ":") %>% 
  map(
    function(x) {
      # Select appropriate variables
      df <- select(healthcare3, all_of(x))
      
      # Extract order such that var with more levels will be first
      index <- df %>% 
        select(-return) %>% 
        map(levels) %>% 
        map_dbl(length) %>% 
        order(decreasing = T)
      
      # Nest data to allow for iterated analysis
      df[c(index, 3)] %>% 
        nest(.by = 1) %>% 
        mutate(variable = names(.)[1]) %>% 
        set_names(c("level", "data", "variable")) 
      }
    ) %>% 
  reduce(add_row) %>% 
  select(variable, level, data) %>% 
  mutate(
    effect = map_chr(data, function(x) {
      names(x) %>% 
        str_c(collapse = " on ")
    }),
    odds = map_dbl(data, odds_ratio),
    cramer = map_dbl(data, cramer_map)
  ) %>% 
  select(-data)

datatable(effects)

```

**Some of the more interesting effects**




```{r}

healthcare3 %>% 
  ggplot() +
  geom_mosaic(aes(x = product(diagnosis, return, dc_result), fill = return)) +
  ggtitle("Rates of return based on diagnosis and discharge result")


```

Overall, being discharged to a destination other than home or self care was associated with a higher proportion of returns. However, more importantly, the diagnosis among those patients who were discharged to home or self care did not affect the rates of return. In contrast, for patients discharged to other types of care, the odds of return were three times smaller for those who had received a diagnosis.




```{r}
healthcare3 %>% 
  ggplot() +
  geom_mosaic(aes(x = product(consult_order, return, dc_result), fill = return)) +
  theme(axis.text.x = element_text(angle = 45))
```

Even bigger differences between effects were observed when discharge result was moderated by consultation. Even though lack of consultation was generally associated with more returns, the results were noticeably more pronounced for those patients who were discharged to other types of care.







```{r fig.asp=1.5}
healthcare3 %>% 
  ggplot() +
  geom_mosaic(aes(x = product(diagnosis, return, gender), fill = return))
```



Having diagnosis differentiated more among men, where it was a stronger predictor of not returning. 


# Model building

  


```{r }

# Exclude unnecessary or repeated variables
healthcare_final <- healthcare3 %>% 
  select(-c(index, charges_int, hour, month, weekday))


# Divide data to train, validation, and test datasets using 60 - 20 - 20 split
set.seed(321)

indexes <- sample(1:3, size = nrow(healthcare_final), replace = T, prob = c(0.6, 0.2, 0.2))

train <- healthcare_final[indexes == 1,]
validation <- healthcare_final[indexes == 2,]
test <- healthcare_final[indexes == 3,]

# Separate features from outcome as required by some of the model functions
X_train <- model.matrix(return ~ ., data = train)[,-1]
Y_train <- as.double(train$return) - 1

X_validation <- model.matrix(return ~ ., data = validation)[,-1]
Y_validation <- as.double(validation$return) - 1

X_test <- model.matrix(return ~ ., data = test)[,-1]
Y_test <- as.double(test$return) - 1


```



```{r}

base_train <- train %>% 
  freq_table(return) %>% 
  arrange(prop) %>% 
  dplyr::slice(1) %>% 
  pull(prop)


```

The misclassification error for the null model is `r base_train`%. 



```{r}


```



## Tree models

Let's start with a basic tree model.

### Basic model


```{r}
fit_tree <- rpart(return ~ ., data = train)



rpart.plot(fit_tree)
```


```{r}
rpart::printcp(fit_tree)
```


```{r}
fit_tree$cptable %>% 
  as_tibble() %>% 
  mutate(
    cs_error = xerror * xstd
  ) %>% 
  own_table()
  



```



```{r}
# Create a function for calculating the classification error rate
mse <- function(x, data, type = "response", outcome = NULL) {
  
  # Calculate predictions based on the model type
  if (class(x) == "ranger") {
    
    pred <- predict(x, data = data)
    y <- pred$predictions
    
    } else {
      
      y <- predict(x, newdata = data, type = type)
      
    }
  
  # Calculate the error rate
  # If xgb.booster model, obtain class based on the probability
  if (class(x) == "xgb.Booster") {
    y <- if_else(y >= 0.5, 1, 0)
    error <- mean(y != outcome) 
    } else {
    error <- mean(y != data$return)
    }
  
  return(round(error, 3))
  
}
```



```{r}
# Set a range of cp values
cps <- seq(0.0001, 0.1, 0.005)

# Create a function for iterated tree modelling
rpart_map <- function(cp) {
  rpart(return ~ ., data = train, cp = cp)
}

# Train many tree models depending on cp
df_trees <- cps %>% 
  enframe() %>% 
  mutate(
    fit_train = map(value, rpart_map)
  )

# Compare miss error on the validation set
df_trees2 <- df_trees %>% 
  mutate(
    miss_error = map_dbl(fit_train, mse, data = validation, type = "class")
  ) 

# Extract the miss error rate for the best of the tree models
best_tree_miss <- min(df_trees2$miss_error)

# Extract the best of the tree models
best_tree_mod <- df_trees2 %>% 
  arrange(miss_error) %>% 
  slice(1) %>% 
  pull(fit_train) %>% 
  {.[[1]]}

# Compare tree models in terms of miss error
df_trees2 %>% 
  select(name, value, miss_error) %>% 
  set_names(c("Model", "Complexity Parameter", "validation classification error")) %>% 
  datatable()

```

Models with a complexity parameter between 0.05 and 0.150 resulted in the smallest prediction error for the validation dataset.

Either increasing or decreasing complexity parameter was associated with a larger prediction error. 

Since among the best models there is a model with the same cp as the model obtained before, the tree structure for the best prediction remains unchanged.



### Bagging and Random forest


For bagging and random forests I am gonna using ranger package, as it allows for parallel computing

```{r}

set.seed(321)

# Fit many models varying the m parameter
forests <- tibble(mtry = 2:(ncol(train) - 1)) %>% 
  mutate(
    model = map(mtry, function(x) {
      ranger(
        return ~ ., 
        data = train, 
        mtry = x,
        importance = "impurity"
        # num.trees = 100 # To safe some computing power
        )})
  )
```



```{r}
# Calculate the MSE for the validation dataset
forests <- forests %>% 
  mutate(
    mse = map_dbl(model, mse, data = validation)
  ) %>% 
  arrange(mse)






```

The best random forest model was achieved using an m = `r forests$mtry[[1]]` number of variables to split across all nodes.


```{r}

importance(forests$model[[1]]) %>% 
  enframe() %>% 
  arrange(desc(value))


```

Charges and age were most predictive of whether one would return to the hospital. 

Interestingly, these were followed by hospital which, given the lack of differences in the initial bivariate analysis, implies noticeable variability in how other variables are distributed across hospitals.
In turn, this suggests that hospitals may adopt distinct approaches to evaluating admissions, potentially affecting patient treatment efficacy and their subsequent return.

Financial class and hour of the day reported a similar level of importance in predicting return to the hospital.


### Boosting


```{r}
# Create DMatrix object
dtrain <- xgb.DMatrix(data = X_train, label = Y_train)

# Create a grid of different parameters combinations
params_grid <- expand_grid(
  eta = seq(0.1, 0.5, 0.1),
  gamma = seq(seq(0.1, 1, 0.2)),
  nrounds = seq(100, 300, 100)
) %>% 
  mutate(
    objective = "binary:logistic",
    nthread = 8,
    max_depth = 6,
    params = pmap(list(objective, nthread, eta, gamma, max_depth), list),
    params = map(params, set_names, nm = c("objective","nthread", "eta", "gamma", "max_depth"))
  )
```


```{r}
# Fit the models
xgboost_models <- params_grid %>% 
  mutate(
    model = map2(
      params, 
      nrounds, 
      function(x, y) {
        xgb.train(params = x, nrounds = y, data = dtrain)
      }
      )
  )
```




```{r}
# Obtain predictions for the validation dataset
xgboost_models2 <- xgboost_models %>% 
  mutate(
    miss_error = map_dbl(model, mse, data = X_validation, outcome = Y_validation)
    ) %>% 
  arrange(miss_error) %>% 
  select(!where(is.list)) 
  

xgboost_models2  %>% 
  select(-c(nthread, objective)) %>% 
  datatable()


```

The boosted models varied in classification error 
from `r min(xgboost_models2$miss_error)`
to `r max(xgboost_models2$miss_error)`.






```{r}


xgb.importance(model = xgboost_models$model[[1]]) %>% 
  datatable()


```

Analyzing the importance of the predictors for the model with the smallest validation classification error, age and charges were observed to have the highest contribution to the prediction, which was similar to the best random forest model, although with the order of the two switched. 

In contrast to random forest, the above were followed by gender and ed result.




## Logistic regression model

We now t

### Lasso regression



```{r}


fit_lasso <- glmnet(X_train, Y_train, family = "binomial")


plot(fit_lasso, xvar = "lambda", label = TRUE)

```



```{r}

# Run 10 fold cross validation lasso regression
fit_lasso_cv <- cv.glmnet(X_train, Y_train, alpha = 1, family = "binomial")
plot(fit_lasso_cv)

# Extract features which coefficients did not reduce to zero
coefs_min <- coef(fit_lasso_cv, s = fit_lasso_cv$lambda.min)[, 1] %>% 
  keep(. != 0) %>% 
  {.[-1]} # Exclude the intercept

coefs_1se <- coef(fit_lasso_cv, s = fit_lasso_cv$lambda.1se)[, 1] %>% 
  keep(. != 0) %>% 
  {.[-1]}

```
Based on the 10-fold cross validation the model with a lambda that produced the minimum average cross-validation error had `r length(coefs_min)` predictos. 

However, applying the one standard error rule, we can get a model with substantially less predictors, that is, `r length(coefs_1se)`.


#### Extracting the predictors to a new logistic model


```{r include=FALSE}
list(coefs_1se, coefs_min) %>% 
  map(names)
```


```{r}

# Extract the pure names of the variables for model building
logistic_formulas <- list(coefs_1se, coefs_min) %>% 
  map(names) %>% 
  map(str_replace, "^(.+?)[A-Z0-9()].*$", "\\1") %>% 
  map(unique) %>% 
  map(str_c,collapse = " + ") 

logistic_formulas2 <- logistic_formulas %>% 
  map(function(x) str_c("return ~ ", x)) 


# Compare models in terms of validation MSE
lasso_models <- logistic_formulas2 %>% 
  map(as.formula) %>% 
  map(glm, family = binomial, data = train) 


lasso_predictions <- lasso_models %>% 
  map(predict, newdata = validation, type = "response") %>% 
  map_dbl(function(x) {
    mean(if_else(x >= 0.5, "Yes", "No") != validation$return)
  }) %>% 
  set_names(
    str_c("Model with ", c(length(coefs_1se), length(coefs_min)), " predicotrs")
  ) 


lasso_predictions %>% 
  enframe() %>% 
  own_table()


```

The two logistic regression models were comparable in terms of validation classification error, although the one with less predictors was slightly better.


#### Supplying the model with interactions




```{r}
# Extract obtained interactions to a vector
interactions_vector <- interactions2 %>% 
 pull(variable) %>% 
 str_replace(":return", "")
```


```{r}
# Assign model outside the list
lasso_1seFit <- glm(return ~ gender + age + race + ethnicity + financial_class + ed_result + acuity_arr + admit_result + consult_order + risk + severity + charges + hour_cat + weekday_cat + month_cat, data = train, family = "binomial")

interaction_models <- list()

# Add interactions to the model
for (i in seq_along(interactions_vector)) {
  lasso_1seFit <- update(lasso_1seFit, str_c(". ~ . + ", interactions_vector[[i]]))
  interaction_models[[i]] <- lasso_1seFit
}
```


```{r}

# Set up all models in a data.frame
interaction_df <- tibble(
  model = interaction_models,
  interactions = seq_along(interactions_vector)
)


# Obtain predictions and miss classification error rate
interaction_df2 <- interaction_df %>% 
  mutate(
    predictions = map(model, predict, newdata = validation, type = "response"),
    error = map_dbl(predictions, class_error, y = validation$return)
  ) %>% 
  arrange(error)




```



```{r}
# All important interactions
# int_formula <-  str_c(
#   logistic_formulas2[[1]],
#   " + ",
#   str_c(interactions_formula, collapse = " + ")
#   ) %>% 
#   as.formula()
```


```{r}



```




```{r}
# train %>% 
#  count(return) 

# Set the starting model
#step_full <- glm(int_formula, data = train, family = "binomial")

# Set the null model
#step_null <- glm(return ~ 1, data = train, family = "binomial")
```


```{r}

# step_model <- stepAIC(step_full, direction = "backward", trace = F)
```


```{r}
# Build the stepwise model
# fit_step <- step(step_null, c(upper = step_full, lower = step_null), direction = "forward")
```


```{r}

# full_predicted <- predict(step_full, newdata = validation, type = "response") %>% 
#   {if_else(. >= 0.5, "Yes", "No")}  
# 
# step_predicted <- predict(step_model, newdata = validation, type = "response") %>% 
#   {if_else(. >= 0.5, "Yes", "No")}  
#   
# 
# full_miss <- mean(validation$return != full_predicted)
# step_miss <- mean(validation$return != step_predicted)




```


Może porównać jeszcze na własną rękę kilka wariantów interaction?
Np. tylko z pierwszymi paroma interakcjami
i np. tylko względem lasso regression



## Best models comparison



```{r}

best_df <- tibble(
  models = list(
    basic_tree = best_tree_mod,  
    random_forest = forests$model[[1]],  
    boosted_tree = xgboost_models$model[[1]]
    ),
  data = list(
    test, test, X_test
  ),
  type = c("class", "response", "response"),
  outcome = list(NULL, NULL, Y_test)
  )


best_df %>% 
  mutate(
    error = pmap_dbl(list(models, data, type, outcome), mse)
  )


```







# Tasks for now

**Crucial questions at this moment of the analysis**

1.    What exactly is the main goal here?
    -   Is pure prediction more important?
    -   Or, is interpretability more important?
2.    In other words, should we aim for fitting the best possible model or finding specific relationships between predictors and the DV, including potential interaction effects?


https://ww2.coastal.edu/kingw/statistics/R-tutorials/loglin.html



# End notes

**Perhaps next time try to first create several versions of the original datasets which will differ in the way certain variables are recoded.**

